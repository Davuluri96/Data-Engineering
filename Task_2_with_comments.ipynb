{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davuluri96/Data-Engineering/blob/main/Task_2_with_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACAL_hN1Q_4c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Maze Environment\n",
        "class MazeEnvironment:\n",
        "    def __init__(self):\n",
        "        # Define the maze layout using a 2D NumPy array (1 = wall, 0 = open space)\n",
        "        self.maze = np.array([\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [1, 0, 0, 1, 0, 1, 1, 1, 0, 1],\n",
        "            [1, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
        "            [1, 0, 0, 0, 0, 1, 0, 0, 1, 1],\n",
        "            [1, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n",
        "            [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
        "            [1, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
        "            [1, 0, 1, 0, 0, 0, 1, 0, 1, 1],\n",
        "            [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        ])\n",
        "\n",
        "        # Define start and goal positions\n",
        "        self.start = (1, 1)  # Start position\n",
        "        self.goal = (8, 8)  # Goal position\n",
        "        self.agent_pos = None  # Agent's current position\n",
        "\n",
        "        # Get maze dimensions\n",
        "        self.rows, self.cols = self.maze.shape\n",
        "\n",
        "        # Define action space size (Up, Down, Left, Right)\n",
        "        self.action_space_size = 4\n",
        "\n",
        "        # Define total number of states (each cell in the maze)\n",
        "        self.state_space_size = self.rows * self.cols\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the agent to the start position\n",
        "        self.agent_pos = self.start\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        # Convert 2D position to a single integer state\n",
        "        return self.agent_pos[0] * self.cols + self.agent_pos[1]\n",
        "\n",
        "    def step(self, action):\n",
        "        # Get the current position of the agent\n",
        "        row, col = self.agent_pos\n",
        "\n",
        "        # Determine the new position based on the action\n",
        "        if action == 0:  # Up\n",
        "            new_row, new_col = row - 1, col\n",
        "        elif action == 1:  # Down\n",
        "            new_row, new_col = row + 1, col\n",
        "        elif action == 2:  # Left\n",
        "            new_row, new_col = row, col - 1\n",
        "        elif action == 3:  # Right\n",
        "            new_row, new_col = row, col + 1\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        # Check if the new position is within bounds and not a wall\n",
        "        if 0 <= new_row < self.rows and 0 <= new_col < self.cols and self.maze[new_row, new_col] == 0:\n",
        "            self.agent_pos = (new_row, new_col)  # Update position if valid\n",
        "\n",
        "        # Define reward system\n",
        "        if self.agent_pos == self.goal:\n",
        "            reward = 10  # Large reward for reaching the goal\n",
        "            done = True  # Episode ends\n",
        "        elif self.maze[new_row, new_col] == 1:  # If hitting a wall\n",
        "            reward = -1  # Small penalty\n",
        "            done = False\n",
        "        else:\n",
        "            reward = -0.1  # Small penalty for each move (encourages shortest path)\n",
        "            done = False\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        # Render the maze environment as text output\n",
        "        for i in range(self.rows):\n",
        "            for j in range(self.cols):\n",
        "                if (i, j) == self.agent_pos:\n",
        "                    print(\"A\", end=\"\")  # Display agent\n",
        "                elif (i, j) == self.goal:\n",
        "                    print(\"G\", end=\"\")  # Display goal\n",
        "                elif self.maze[i, j] == 1:\n",
        "                    print(\"#\", end=\"\")  # Display wall\n",
        "                else:\n",
        "                    print(\".\", end=\"\")  # Display open space\n",
        "            print()\n",
        "        print()\n",
        "\n",
        "# Q-Learning Agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, learning_rate=0.1, discount_factor=0.9, epsilon=1.0, epsilon_decay=0.001):\n",
        "        # Initialize Q-table with zeros\n",
        "        self.q_table = np.zeros((state_space_size, action_space_size))\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.lr = learning_rate  # Learning rate\n",
        "        self.gamma = discount_factor  # Discount factor for future rewards\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.epsilon_decay = epsilon_decay  # Decay rate for epsilon\n",
        "        self.action_space_size = action_space_size\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(self.action_space_size)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])  # Exploit best known action\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        # Update Q-table using the Bellman equation\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :])\n",
        "        self.q_table[state, action] += self.lr * (reward + self.gamma * self.q_table[next_state, best_next_action] - self.q_table[state, action])\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        # Decay epsilon to reduce exploration over time\n",
        "        self.epsilon = max(0.01, self.epsilon - self.epsilon_decay)  # Ensure epsilon doesn't go below 0.01\n",
        "\n",
        "# --- Training Loop ---\n",
        "env = MazeEnvironment()\n",
        "agent = QLearningAgent(env.state_space_size, env.action_space_size)\n",
        "\n",
        "num_episodes = 1000  # Number of training episodes\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()  # Reset environment\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)  # Choose action\n",
        "        next_state, reward, done = env.step(action)  # Take step in environment\n",
        "        agent.update_q_table(state, action, reward, next_state)  # Update Q-table\n",
        "        state = next_state  # Move to next state\n",
        "\n",
        "    agent.decay_epsilon()  # Decay exploration rate\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"--- Training complete, evaluating agent ---\")\n",
        "agent.epsilon = 0  # Disable exploration during evaluation\n",
        "num_test_episodes = 10\n",
        "success_count = 0\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        state = next_state\n",
                 # env.render() #Uncomment to render eachÂ step
        
        "    if env.agent_pos == env.goal:\n",
        "        success_count += 1\n",
        "\n",
        "# Print final success rate\n",
        "success_rate = success_count / num_test_episodes\n",
        "print(f\"Success rate: {success_rate:.2f}\")\n"
      ]
    }
  ]
}
